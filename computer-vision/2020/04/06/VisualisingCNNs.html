<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Paper Reading - Visualizing and Understanding Convolutional Networks | Alephthoughts</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Paper Reading - Visualizing and Understanding Convolutional Networks" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="What do CNNs see?" />
<meta property="og:description" content="What do CNNs see?" />
<link rel="canonical" href="https://alephthoughts.com/computer-vision/2020/04/06/VisualisingCNNs.html" />
<meta property="og:url" content="https://alephthoughts.com/computer-vision/2020/04/06/VisualisingCNNs.html" />
<meta property="og:site_name" content="Alephthoughts" />
<meta property="og:image" content="https://alephthoughts.com/images/mathew.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-04-06T00:00:00-05:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://alephthoughts.com/images/mathew.jpg" />
<meta property="twitter:title" content="Paper Reading - Visualizing and Understanding Convolutional Networks" />
<script type="application/ld+json">
{"url":"https://alephthoughts.com/computer-vision/2020/04/06/VisualisingCNNs.html","@type":"BlogPosting","headline":"Paper Reading - Visualizing and Understanding Convolutional Networks","dateModified":"2020-04-06T00:00:00-05:00","datePublished":"2020-04-06T00:00:00-05:00","image":"https://alephthoughts.com/images/mathew.jpg","mainEntityOfPage":{"@type":"WebPage","@id":"https://alephthoughts.com/computer-vision/2020/04/06/VisualisingCNNs.html"},"description":"What do CNNs see?","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://alephthoughts.com/feed.xml" title="Alephthoughts" /><!-- the google_analytics_id gets auto inserted from the config file -->



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-116353462-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-116353462-1');
</script>


<link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Alephthoughts</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/now/">Now</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Paper Reading - Visualizing and Understanding Convolutional Networks</h1><p class="page-description">What do CNNs see?</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-04-06T00:00:00-05:00" itemprop="datePublished">
        Apr 6, 2020
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      3 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#computer-vision">computer-vision</a>
        
      
      </p>
    

    
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-04-06-VisualisingCNNs.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The first paper I chose to write about is one of the most famous papers in the Deep Learning community titled, <a href="https://arxiv.org/abs/1311.2901">Visualizing and understanding convolutional networks</a> by <a href="https://www.matthewzeiler.com/">M. Zeiler</a>, <a href="https://cs.nyu.edu/~fergus/pmwiki/pmwiki.php">R. Fergus</a>. Appeared in 2013 this paper remains popular as it introduced some of the most fundamental ideas to aid our understanding of convolutional neural networks and transfer learning.
Convolutional neural networks have performed exceedingly well over time on image related tasks. The main objective of this paper is enquire about :-</p>
<ol>
<li>Why convolutional networks perform so well on image classification tasks?</li>
<li>How the convolutional model architectures can be improved?</li>
</ol>
<p>The most important ideas introduced in this paper are the following:</p>
<ol>
<li>Deconvnets and Convnet Visualization</li>
<li>Convnet Explainability</li>
<li>Transfer Learning</li>
</ol>
<p>Before looking into these ideas in a little detail, let's first discuss the convnet model architecture studied and used in this paper. The standard operation of a convnet is to map colored two dimensional images to a probability vector over different classes i.e. it takes an image as input and returns a vector of probabilities(adding to 1) of the image belonging to each of the classes. Each layer of the convenet consists of:</p>
<ul>
<li><a href="https://www.superdatascience.com/blogs/convolutional-neural-networks-cnn-step-1-convolution-operation">convolution</a> of the previous layer output (for the first layer it's the input image) with a set of filters (parameters)</li>
<li>rectified linear units <a href="https://www.kaggle.com/dansbecker/rectified-linear-units-relu-in-deep-learning">relu</a>, which is essentially replacing all negative activations with 0</li>
<li><a href="https://computersciencewiki.org/index.php/Max-pooling_/_Pooling">max pooling</a> </li>
<li>normalizing the activations by <a href="https://towardsdatascience.com/difference-between-local-response-normalization-and-batch-normalization-272308c034ac">local response normalization</a></li>
</ul>
<p>The top few layers of the convnet are the fully-connected layers follwed by a <a href="https://www.pyimagesearch.com/2016/09/12/softmax-classifiers-explained/">softmax classifier</a> at the end.</p>
<p>Now let's look at the important ideas discussed in this paper:</p>
<h2 id="Visualizing-convolutional-networks-with-Deconvnet">Visualizing convolutional networks with Deconvnet<a class="anchor-link" href="#Visualizing-convolutional-networks-with-Deconvnet"> </a></h2><p>The process of visualizing a convnet involves identifying the cause of each activation by mapping the activations back to input pixel space with the aid of a <a href="https://matthewzeiler.com/mattzeiler/adaptivedeconvolutional.pdf">Deconvolutional Network</a> (deconvnet). A deconvnet can be thought of as a convnet but in reverse. It is attached to each of the convnet layers in order to probe the activations. In order to examine a given activation, all other activations in the conenet layer are set to zero and the feature maps are passed to the attached deconvnet layer. Then following operations are performed repeatedly until input pixel space is reconstructed:</p>
<ul>
<li><strong>Unpooling</strong> : Since max pooling operation is non-invertible, an approximate inverse is obtained by restoring the activations at the locations of maximum activation selection.</li>
<li><strong>Rectification</strong> : The reconstructed signals are passed through a relu non-linearity.</li>
<li><strong>Filtering</strong> : In the deconvnet each convnet filter is transposed (flipped vertically and horizontally) and applied to the rectified activations.
<img src="/alephthoughts/images/copied_from_nb/my_icons/deconvnet.jpeg" alt="Deconvnet attached to a convnet &gt;&lt;" /></li>
</ul>
<h3 id="Convnet-Visualization">Convnet Visualization<a class="anchor-link" href="#Convnet-Visualization"> </a></h3><p>The aforementioned approach is used to visualize the top 9 strongest activations for a given feature map. These activations are projected to pixel space and shown alongside corresponding image patches. Let's see each layer one by one:</p>
<h4 id="Layer-2:">Layer 2:<a class="anchor-link" href="#Layer-2:"> </a></h4><p>Layer 2 corresponds to corners and other edge/color conjuctions.
<img src="/alephthoughts/images/copied_from_nb/my_icons/Layer2.png" alt="" title="Layer 2" /></p>
<h4 id="Layer-3:">Layer 3:<a class="anchor-link" href="#Layer-3:"> </a></h4><p>Layer 3 captures complex patterns such as mesh patterns, texts etc.
<img src="/alephthoughts/images/copied_from_nb/my_icons/Layer3.png" alt="" title="Layer 3" /></p>
<h4 id="Layer-4:">Layer 4:<a class="anchor-link" href="#Layer-4:"> </a></h4><p>Layer 4 shows further variations and is able to classify objects in the picture such as dog faces, bird legs etc.
<img src="/alephthoughts/images/copied_from_nb/my_icons/Layer4.png" alt="" title="Layer 4" /></p>
<h4 id="Layer-5:">Layer 5:<a class="anchor-link" href="#Layer-5:"> </a></h4><p>Layer 5 shows complete objects such as keyboards, dogs etc.
<img src="/alephthoughts/images/copied_from_nb/my_icons/Layer5.png" alt="" title="Layer 5" /></p>
<h3 id="Key-Insights">Key Insights<a class="anchor-link" href="#Key-Insights"> </a></h3><ul>
<li>The lower layers of the model converge within few epochs and upper layers fully converge only after a considerable number of epochs. This suggests it might be beneficial to train different layers at different learning rates.</li>
<li>By systematically occluding different portions of the image with a grey square and monitoring the output of the classifier, it was observed that the model was able to localize the objects in the scene (explainabilty).</li>
<li>Overall depth of the model is important for obtaining good performance. Changing the size of the fully connected layers makes little difference to perfor- mance. However, increasing the size of the middle convolution layers  give a useful gain in performance. But in- creasing these, while also enlarging the fully connected layers results in overfitting.</li>
<li>Keeping intial layers trained on ImageNet fixed and training a new softmax classifier of appropriate size using training images from Caltech-256, Caltech-101, PASCAL VOC 2012 datasets resulted in faster training and better results. (Transfer Learning)</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><div class="flash">
    <svg class="octicon octicon-info" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z"></path></svg>
    <strong>Note: </strong>All images belong to M. Zeiler and R. Fergus. Zeiler&#8217;s image taken from <a href="https://www.matthewzeiler.com/">matthewzeiler.com</a>.
</div></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><script type="text/javascript" src="https://cdnjs.buymeacoffee.com/1.0.0/button.prod.min.js" data-name="bmc-button" data-slug="alephthoughts" data-color="#FFDD00" data-emoji="" data-font="Cookie" data-text="Buy me a coffee" data-outline-color="#000000" data-font-color="#000000" data-coffee-color="#ffffff"></script></p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="alephthoughts/alephthoughts"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/computer-vision/2020/04/06/VisualisingCNNs.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Using Data Science to solve interesting problems.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/alephthoughts" target="_blank" title="alephthoughts"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/alephthoughts" target="_blank" title="alephthoughts"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
